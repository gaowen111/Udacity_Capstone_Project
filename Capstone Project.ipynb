{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Project Title\n",
    "### Data Engineering Capstone Project\n",
    "\n",
    "#### Project Summary\n",
    "--describe your project at a high level--\n",
    "\n",
    "The project follows the follow steps:\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Do all imports and installs here\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf, col, row_number, lit\n",
    "#from pyspark.sql.functions import year, month, dayofmonth, hour, weekofyear, dayofweek, date_format\n",
    "from pyspark.sql.types import TimestampType\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import types \n",
    "import configparser\n",
    "import psycopg2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 1: Scope the Project and Gather Data\n",
    "\n",
    "#### Scope \n",
    "Explain what you plan to do in the project in more detail. What data do you use? What is your end solution look like? What tools did you use? etc>\n",
    "1. Data: The scope of this project, is to gather immigration data based on state level, to count how many immigration happened for 2016 in each month, and provide the possibility to analyze how the number is related to the state population/ household size/ median age etc.\n",
    "2. End solution: I collected data using pyspark as it is a big data set, and then do data cleansing+transformation using pyspark dataframe. After that, I uploaded the datasets to AWS S3 to store the data, and then I copied the data from S3 to AWS Redshift, to have clean data structure on a data warehouse, and the data would be ready to use to perform analysis.\n",
    "3. Tools included: Spark, AWS EMR cluster, S3 and Redshift.\n",
    "\n",
    "#### Describe and Gather Data \n",
    "Describe the data sets you're using. Where did it come from? What type of information is included? \n",
    "1. Dataset 1: SAS I94 Immigration Data: This data comes from the US National Tourism and Trade Office. Data provided by udacity course workspace. The dataset includes info about when and where has happened immigration, and also some info like gender, visa type on the immigrants data.\n",
    "2. Dataset 2: U.S. City Demographic Data: This data comes from OpenSoft. Data provided by udacity course workspace. The dataset includes info about states in the US, and how much the population is/ what the median age(etc) for each state/city.\n",
    "3. Both dataset have state_code(i94addr in immigration dataset), that can be used to join the 2 datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 1. Read immigration data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Read immigration sample data and look at the data structure.\n",
    "df = pd.read_csv(\"immigration_data_sample.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>cicid</th>\n",
       "      <th>i94yr</th>\n",
       "      <th>i94mon</th>\n",
       "      <th>i94cit</th>\n",
       "      <th>i94res</th>\n",
       "      <th>i94port</th>\n",
       "      <th>arrdate</th>\n",
       "      <th>i94mode</th>\n",
       "      <th>i94addr</th>\n",
       "      <th>...</th>\n",
       "      <th>entdepu</th>\n",
       "      <th>matflag</th>\n",
       "      <th>biryear</th>\n",
       "      <th>dtaddto</th>\n",
       "      <th>gender</th>\n",
       "      <th>insnum</th>\n",
       "      <th>airline</th>\n",
       "      <th>admnum</th>\n",
       "      <th>fltno</th>\n",
       "      <th>visatype</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2027561</td>\n",
       "      <td>4084316.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>209.0</td>\n",
       "      <td>209.0</td>\n",
       "      <td>HHW</td>\n",
       "      <td>20566.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>HI</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>1955.0</td>\n",
       "      <td>07202016</td>\n",
       "      <td>F</td>\n",
       "      <td>NaN</td>\n",
       "      <td>JL</td>\n",
       "      <td>5.658267e+10</td>\n",
       "      <td>00782</td>\n",
       "      <td>WT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2171295</td>\n",
       "      <td>4422636.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>582.0</td>\n",
       "      <td>582.0</td>\n",
       "      <td>MCA</td>\n",
       "      <td>20567.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>TX</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>1990.0</td>\n",
       "      <td>10222016</td>\n",
       "      <td>M</td>\n",
       "      <td>NaN</td>\n",
       "      <td>*GA</td>\n",
       "      <td>9.436200e+10</td>\n",
       "      <td>XBLNG</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>589494</td>\n",
       "      <td>1195600.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>148.0</td>\n",
       "      <td>112.0</td>\n",
       "      <td>OGG</td>\n",
       "      <td>20551.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>FL</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>1940.0</td>\n",
       "      <td>07052016</td>\n",
       "      <td>M</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LH</td>\n",
       "      <td>5.578047e+10</td>\n",
       "      <td>00464</td>\n",
       "      <td>WT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2631158</td>\n",
       "      <td>5291768.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>297.0</td>\n",
       "      <td>297.0</td>\n",
       "      <td>LOS</td>\n",
       "      <td>20572.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>CA</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>1991.0</td>\n",
       "      <td>10272016</td>\n",
       "      <td>M</td>\n",
       "      <td>NaN</td>\n",
       "      <td>QR</td>\n",
       "      <td>9.478970e+10</td>\n",
       "      <td>00739</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3032257</td>\n",
       "      <td>985523.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>111.0</td>\n",
       "      <td>111.0</td>\n",
       "      <td>CHM</td>\n",
       "      <td>20550.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NY</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>1997.0</td>\n",
       "      <td>07042016</td>\n",
       "      <td>F</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.232257e+10</td>\n",
       "      <td>LAND</td>\n",
       "      <td>WT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0      cicid   i94yr  i94mon  i94cit  i94res i94port  arrdate  \\\n",
       "0     2027561  4084316.0  2016.0     4.0   209.0   209.0     HHW  20566.0   \n",
       "1     2171295  4422636.0  2016.0     4.0   582.0   582.0     MCA  20567.0   \n",
       "2      589494  1195600.0  2016.0     4.0   148.0   112.0     OGG  20551.0   \n",
       "3     2631158  5291768.0  2016.0     4.0   297.0   297.0     LOS  20572.0   \n",
       "4     3032257   985523.0  2016.0     4.0   111.0   111.0     CHM  20550.0   \n",
       "\n",
       "   i94mode i94addr    ...     entdepu  matflag  biryear   dtaddto  gender  \\\n",
       "0      1.0      HI    ...         NaN        M   1955.0  07202016       F   \n",
       "1      1.0      TX    ...         NaN        M   1990.0  10222016       M   \n",
       "2      1.0      FL    ...         NaN        M   1940.0  07052016       M   \n",
       "3      1.0      CA    ...         NaN        M   1991.0  10272016       M   \n",
       "4      3.0      NY    ...         NaN        M   1997.0  07042016       F   \n",
       "\n",
       "  insnum airline        admnum  fltno  visatype  \n",
       "0    NaN      JL  5.658267e+10  00782        WT  \n",
       "1    NaN     *GA  9.436200e+10  XBLNG        B2  \n",
       "2    NaN      LH  5.578047e+10  00464        WT  \n",
       "3    NaN      QR  9.478970e+10  00739        B2  \n",
       "4    NaN     NaN  4.232257e+10   LAND        WT  \n",
       "\n",
       "[5 rows x 29 columns]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Read AWS configuration\n",
    "config = configparser.ConfigParser()\n",
    "config.read('dl.cfg')\n",
    "os.environ['AWS_ACCESS_KEY_ID']=config['AWS']['AWS_ACCESS_KEY_ID']\n",
    "os.environ['AWS_SECRET_ACCESS_KEY']=config['AWS']['AWS_SECRET_ACCESS_KEY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Build spark session\n",
    "\n",
    "spark = SparkSession.builder.\\\n",
    "config(\"spark.jars.repositories\", \"https://repos.spark-packages.org/\").\\\n",
    "config(\"spark.jars.packages\", \"saurfang:spark-sas7bdat:2.0.0-s_2.11,org.apache.hadoop:hadoop-aws:2.7.0\").\\\n",
    "enableHiveSupport().getOrCreate()\n",
    "# df_spark = spark.read.format('com.github.saurfang.sas.spark').load('../../data/18-83510-I94-Data-2016/i94_jan16_sub.sas7bdat')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2847924\n",
      "5418467\n",
      "8575539\n",
      "11671852\n",
      "15116101\n",
      "18691090\n",
      "22956121\n",
      "27059691\n",
      "30793477\n",
      "34442613\n",
      "37357539\n",
      "40790529\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "40790529"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read all months immigration data\n",
    "from functools import reduce  # For Python 3.x\n",
    "from pyspark.sql import DataFrame\n",
    "\n",
    "    \n",
    "def unionAll(*dfs):\n",
    "    return reduce(DataFrame.unionAll, dfs)\n",
    "i = 0\n",
    "files = ['../../data/18-83510-I94-Data-2016/i94_jan16_sub.sas7bdat',\n",
    "         '../../data/18-83510-I94-Data-2016/i94_feb16_sub.sas7bdat',\n",
    "         '../../data/18-83510-I94-Data-2016/i94_mar16_sub.sas7bdat',\n",
    "         '../../data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat',\n",
    "         '../../data/18-83510-I94-Data-2016/i94_may16_sub.sas7bdat',\n",
    "         '../../data/18-83510-I94-Data-2016/i94_jun16_sub.sas7bdat',\n",
    "         '../../data/18-83510-I94-Data-2016/i94_jul16_sub.sas7bdat',\n",
    "         '../../data/18-83510-I94-Data-2016/i94_aug16_sub.sas7bdat',\n",
    "         '../../data/18-83510-I94-Data-2016/i94_sep16_sub.sas7bdat',\n",
    "         '../../data/18-83510-I94-Data-2016/i94_oct16_sub.sas7bdat',\n",
    "         '../../data/18-83510-I94-Data-2016/i94_nov16_sub.sas7bdat',\n",
    "         '../../data/18-83510-I94-Data-2016/i94_dec16_sub.sas7bdat']\n",
    "for file in files:\n",
    "    if(i==0):\n",
    "        df = spark.read.format('com.github.saurfang.sas.spark').load(file)\n",
    "        cols= df.columns\n",
    "        print(df.count())\n",
    "    if(i>0):\n",
    "        df = unionAll(df,spark.read.format('com.github.saurfang.sas.spark').load(file).select([col for col in cols]))\n",
    "        print(df.count())\n",
    "    i = i+1\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9997"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9997"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read temp aggregated immigration data from local file, as the loading time for above full data set is too long.\n",
    "# df = spark.read.parquet(\"sas_data_new\")\n",
    "# df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method DataFrame.describe of DataFrame[i94yr: double, i94mon: double, i94addr: string, gender: string, count: bigint]>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 2. Read demographic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Read data from project workspace\n",
    "dem_df = spark.read.csv(\"us-cities-demographics.csv\", sep=';', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(City='Silver Spring', State='Maryland', Median Age='33.8', Male Population='40601', Female Population='41862', Total Population='82463', Number of Veterans='1562', Foreign-born='30908', Average Household Size='2.6', State Code='MD', Race='Hispanic or Latino', Count='25924'),\n",
       " Row(City='Quincy', State='Massachusetts', Median Age='41.0', Male Population='44129', Female Population='49500', Total Population='93629', Number of Veterans='4147', Foreign-born='32935', Average Household Size='2.39', State Code='MA', Race='White', Count='58723'),\n",
       " Row(City='Hoover', State='Alabama', Median Age='38.5', Male Population='38040', Female Population='46799', Total Population='84839', Number of Veterans='4819', Foreign-born='8229', Average Household Size='2.58', State Code='AL', Race='Asian', Count='4759'),\n",
       " Row(City='Rancho Cucamonga', State='California', Median Age='34.5', Male Population='88127', Female Population='87105', Total Population='175232', Number of Veterans='5821', Foreign-born='33878', Average Household Size='3.18', State Code='CA', Race='Black or African-American', Count='24437'),\n",
       " Row(City='Newark', State='New Jersey', Median Age='34.6', Male Population='138040', Female Population='143873', Total Population='281913', Number of Veterans='5829', Foreign-born='86253', Average Household Size='2.73', State Code='NJ', Race='White', Count='76402')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dem_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2891"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dem_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method DataFrame.describe of DataFrame[City: string, State: string, Median Age: string, Male Population: string, Female Population: string, Total Population: string, Number of Veterans: string, Foreign-born: string, Average Household Size: string, State Code: string, Race: string, Count: string]>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dem_df.describe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 2: Explore and Assess the Data\n",
    "#### Explore the Data \n",
    "Identify data quality issues, like missing values, duplicate data, etc.\n",
    "\n",
    "#### Cleaning Steps\n",
    "Document steps necessary to clean the data.\n",
    "1. Drop duplicated data for both datasets.\n",
    "2. As the main join would be performed on state_code/i94addr, so need to drop all null values in both datasets to avoid invalid join.\n",
    "3. Aggregation on dataset of immigration: Check how many immigrations happened group by year, month, state code and gender\n",
    "4. Change column types in both datasets to match the real values in the columns.\n",
    "5. Change column names to a good format, as S3 doesn't allow column names to have space in between."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 1. Drop duplicates for immigration data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Drop duplicate data in immigration dataset\n",
    "df = df.dropDuplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Drop null values of stats code in immigration dataset\n",
    "df = df.filter(df['i94addr'].isNotNull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Change column type\n",
    "df = df.withColumn(\"i94yr\", df[\"i94yr\"].cast(types.IntegerType())).withColumn(\"i94mon\", df[\"i94mon\"].cast(types.IntegerType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method DataFrame.describe of DataFrame[i94yr: int, i94mon: int, i94addr: string, gender: string, count: bigint]>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+-------+------+-----+\n",
      "| i94yr|i94mon|i94addr|gender|count|\n",
      "+------+------+-------+------+-----+\n",
      "|2016.0|   1.0|     US|     F|    1|\n",
      "|2016.0|   1.0|     FI|     M|    1|\n",
      "|2016.0|   1.0|     CU|     F|    1|\n",
      "|2016.0|   2.0|     ME|     M|    1|\n",
      "|2016.0|   2.0|     IQ|     F|    1|\n",
      "|2016.0|   2.0|     TS|     F|    1|\n",
      "|2016.0|   3.0|     AR|     M|    1|\n",
      "|2016.0|   3.0|     NJ|     U|    1|\n",
      "|2016.0|   4.0|     VA|     M|    1|\n",
      "|2016.0|   4.0|     BO|     M|    1|\n",
      "|2016.0|   4.0|     TC|     M|    1|\n",
      "|2016.0|   4.0|     NM|     U|    1|\n",
      "|2016.0|   5.0|     CD|     F|    1|\n",
      "|2016.0|   5.0|     CE|     F|    1|\n",
      "|2016.0|   5.0|     WV|     X|    1|\n",
      "|2016.0|   7.0|     UH|     F|    1|\n",
      "|2016.0|   8.0|     CT|     M|    1|\n",
      "|2016.0|   8.0|     NT|     M|    1|\n",
      "|2016.0|   9.0|     DL|     F|    1|\n",
      "|2016.0|   9.0|      A|     M|    1|\n",
      "+------+------+-------+------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Data aggregation for immigration dataset\n",
    "df = df.groupBy('i94yr','i94mon','i94addr','gender').count()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9997"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Save aggregated immigration data to local path to read next time\n",
    "#df.write.save(\"sas_data_new\", format = \"parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2891"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop duplicate data in demographics dataset\n",
    "dem_df = dem_df.dropDuplicates()\n",
    "dem_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2891"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop null state code values in demographics dataset\n",
    "dem_df = dem_df.filter(dem_df['State Code'].isNotNull())\n",
    "dem_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Rename column names to better format\n",
    "dem_df = dem_df.withColumnRenamed(\"Median Age\",\"MedianAge\").withColumnRenamed(\"Male Population\",\"MalePopulation\").withColumnRenamed(\"Female Population\",\"FemalePopulation\")\\\n",
    ".withColumnRenamed(\"Total Population\",\"TotalPopulation\").withColumnRenamed(\"Number of Veterans\",\"NumberofVeterans\").withColumnRenamed(\"Foreign-born\",\"ForeignBorn\")\\\n",
    ".withColumnRenamed(\"Average Household Size\",\"AverageHouseholdSize\").withColumnRenamed(\"State Code\",\"StateCode\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Change column types according to values\n",
    "dem_df = dem_df.withColumn(\"MedianAge\", dem_df[\"MedianAge\"].cast(types.DoubleType())).withColumn(\"MalePopulation\", dem_df[\"MalePopulation\"].cast(types.LongType()))\\\n",
    ".withColumn(\"FemalePopulation\", dem_df[\"FemalePopulation\"].cast(types.LongType())).withColumn(\"TotalPopulation\", dem_df[\"TotalPopulation\"].cast(types.LongType()))\\\n",
    ".withColumn(\"NumberofVeterans\", dem_df[\"NumberofVeterans\"].cast(types.LongType())).withColumn(\"ForeignBorn\", dem_df[\"ForeignBorn\"].cast(types.LongType()))\\\n",
    ".withColumn(\"AverageHouseholdSize\", dem_df[\"AverageHouseholdSize\"].cast(types.DoubleType())).withColumn(\"Count\", dem_df[\"Count\"].cast(types.LongType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method DataFrame.describe of DataFrame[City: string, State: string, MedianAge: double, MalePopulation: bigint, FemalePopulation: bigint, TotalPopulation: bigint, NumberofVeterans: bigint, ForeignBorn: bigint, AverageHouseholdSize: double, StateCode: string, Race: string, Count: bigint]>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dem_df.describe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 3: Define the Data Model\n",
    "#### 3.1 Conceptual Data Model\n",
    "Map out the conceptual data model and explain why you chose that model\n",
    "1. Fact table: Immigration cound based on year month, and gender.\n",
    "2. Dimension table: Cities, states in the US and the population, median age,NumberofVeterans, ForeignBorn, AverageHouseholdSize. I kept dimension table at city level, to have more flexibility in the future, but when doing joins in between, I used states level aggregation.\n",
    "3. 2 tables joined by i94addr in Immigration data and state code in demographics data.\n",
    "\n",
    "#### 3.2 Mapping Out Data Pipelines\n",
    "List the steps necessary to pipeline the data into the chosen data model\n",
    "1. Upload cleaned dataset to S3 using spark + EMR cluster.\n",
    "2. Create tables on Redshift.\n",
    "3. Copy data from S3 to Redshift tables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 4: Run Pipelines to Model the Data \n",
    "#### 4.1 Create the data model\n",
    "Build the data pipelines to create the data model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 1. Upload data to S3 using EMR cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Define S3 bucket path\n",
    "output_data = \"s3a://udacityprojectwencapstone/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(i94yr=2016, i94mon=10, i94addr='SF', gender=None, count=47)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Write immigration data to S3 in parquet format\n",
    "df.write.mode(\"overwrite\").parquet(output_data + \"immigration/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Write demographic data to S3 in parquet format\n",
    "dem_df.write.mode(\"overwrite\").parquet(output_data + \"demographic/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 2. S3 to Redshift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Read configuration of AWS Redshift\n",
    "config = configparser.ConfigParser()\n",
    "config.read('dl.cfg')\n",
    "# Build connection to redshift and be ready to execute operations\n",
    "conn = psycopg2.connect(\"host={} dbname={} user={} password={} port={}\".format(*config['CLUSTER'].values()))\n",
    "cur = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Immigration table drop/creation sqls\n",
    "immigration_table_drop = \"Drop table if exists immigration\"\n",
    "immigration_table_create= (\"\"\"\n",
    "CREATE TABLE immigration\n",
    "(\n",
    "  immi_id  bigint identity(0, 1) primary key,\n",
    "  i94yr integer NULL,\n",
    "  i94mon       integer NULL,\n",
    "  i94addr       VARCHAR(64) NULL,\n",
    "  gender       VARCHAR(64) NULL,\n",
    "  count     BIGINT NULL\n",
    ");\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Demographic table drop/creation sqls\n",
    "demographic_table_drop = \"Drop table if exists demographic\"\n",
    "demographic_table_create= (\"\"\"\n",
    "CREATE TABLE demographic \n",
    "(\n",
    "  city_id   bigint identity(0, 1) primary key,\n",
    "  city      VARCHAR(256) NULL,\n",
    "  state     VARCHAR(256) NULL,\n",
    "  median_age    FLOAT NULL,\n",
    "  male_population     BIGINT NULL,\n",
    "  female_population     BIGINT NULL,\n",
    "  total_population BIGINT NULL,\n",
    "  number_of_veterans BIGINT NULL,\n",
    "  foreign_born BIGINT NULL,\n",
    "  average_household_size FLOAT NULL,\n",
    "  state_code VARCHAR(256) NULL,\n",
    "  race VARCHAR(256) NULL,\n",
    "  count BIGINT NULL\n",
    ");\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "drop_table_queries = [immigration_table_drop, demographic_table_drop]\n",
    "create_table_queries = [immigration_table_create, demographic_table_create]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Excecute tables drop\n",
    "for query in drop_table_queries:\n",
    "    cur.execute(query)\n",
    "    conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Excecute tables creation\n",
    "for query in create_table_queries:\n",
    "    cur.execute(query)\n",
    "    conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Read S3 path and IAM role\n",
    "IMMI_DATA=config.get('S3','IMMI_DATA')\n",
    "DEM_DATA=config.get('S3','DEM_DATA')\n",
    "DWH_ROLE_ARN=config.get('IAM_ROLE','ARN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# S3 copy to redshift command\n",
    "immigration_copy = \"\"\"\n",
    "    copy immigration from {}\n",
    "    credentials 'aws_iam_role={}'\n",
    "    FORMAT AS PARQUET;\n",
    "\"\"\".format(IMMI_DATA,DWH_ROLE_ARN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# S3 copy to redshift command\n",
    "demographic_copy = \"\"\"\n",
    "    copy demographic from {}\n",
    "    credentials 'aws_iam_role={}'\n",
    "    FORMAT AS PARQUET;\n",
    "\"\"\".format(DEM_DATA,DWH_ROLE_ARN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Execute S3 copy\n",
    "cur.execute(immigration_copy)\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Execute S3 copy\n",
    "cur.execute(demographic_copy)\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Closed connection to redshift\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.2 Data Quality Checks\n",
    "Explain the data quality checks you'll perform to ensure the pipeline ran as expected. These could include:\n",
    " * Integrity constraints on the relational database (e.g., unique key, data type, etc.)\n",
    " * Unit tests for the scripts to ensure they are doing the right thing\n",
    " * Source/Count checks to ensure completeness\n",
    " \n",
    " 1. Primary key has been created for incremental values in the table creation.\n",
    " 2. SQL Qyeries about possible analysis.\n",
    " 3. Count table records, should be more than 0.\n",
    " \n",
    "Run Quality Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Perform quality checks here\n",
    "%load_ext sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Connected: awsuser@dev'"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%sql postgresql://awsuser:Passw0rd@redshift-cluster-1.ctagmngdnkvt.us-west-2.redshift.amazonaws.com:5439/dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * postgresql://awsuser:***@redshift-cluster-1.ctagmngdnkvt.us-west-2.redshift.amazonaws.com:5439/dev\n",
      "1 rows affected.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <tr>\n",
       "        <th>count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>9997</td>\n",
       "    </tr>\n",
       "</table>"
      ],
      "text/plain": [
       "[(9997,)]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check if records >0 in immigration table\n",
    "%sql SELECT COUNT(*) FROM IMMIGRATION;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * postgresql://awsuser:***@redshift-cluster-1.ctagmngdnkvt.us-west-2.redshift.amazonaws.com:5439/dev\n",
      "1 rows affected.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <tr>\n",
       "        <th>count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>2891</td>\n",
       "    </tr>\n",
       "</table>"
      ],
      "text/plain": [
       "[(2891,)]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check if records >0 in demographic table\n",
    "%sql SELECT COUNT(*) FROM DEMOGRAPHIC;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * postgresql://awsuser:***@redshift-cluster-1.ctagmngdnkvt.us-west-2.redshift.amazonaws.com:5439/dev\n",
      "12 rows affected.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <tr>\n",
       "        <th>i94yr</th>\n",
       "        <th>i94mon</th>\n",
       "        <th>sum</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>2016</td>\n",
       "        <td>7</td>\n",
       "        <td>4075542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>2016</td>\n",
       "        <td>8</td>\n",
       "        <td>3919108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>2016</td>\n",
       "        <td>9</td>\n",
       "        <td>3567083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>2016</td>\n",
       "        <td>10</td>\n",
       "        <td>3487868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>2016</td>\n",
       "        <td>6</td>\n",
       "        <td>3388925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>2016</td>\n",
       "        <td>5</td>\n",
       "        <td>3271548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>2016</td>\n",
       "        <td>12</td>\n",
       "        <td>3266165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>2016</td>\n",
       "        <td>3</td>\n",
       "        <td>2993671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>2016</td>\n",
       "        <td>4</td>\n",
       "        <td>2943721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>2016</td>\n",
       "        <td>11</td>\n",
       "        <td>2756885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>2016</td>\n",
       "        <td>1</td>\n",
       "        <td>2670795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>2016</td>\n",
       "        <td>2</td>\n",
       "        <td>2421292</td>\n",
       "    </tr>\n",
       "</table>"
      ],
      "text/plain": [
       "[(2016, 7, 4075542),\n",
       " (2016, 8, 3919108),\n",
       " (2016, 9, 3567083),\n",
       " (2016, 10, 3487868),\n",
       " (2016, 6, 3388925),\n",
       " (2016, 5, 3271548),\n",
       " (2016, 12, 3266165),\n",
       " (2016, 3, 2993671),\n",
       " (2016, 4, 2943721),\n",
       " (2016, 11, 2756885),\n",
       " (2016, 1, 2670795),\n",
       " (2016, 2, 2421292)]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Possible analysis 1- See in 2016, which month has highest immigration.\n",
    "%sql select i94yr, i94mon, sum(count) from immigration group by i94yr, i94mon order by sum(count) desc;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * postgresql://awsuser:***@redshift-cluster-1.ctagmngdnkvt.us-west-2.redshift.amazonaws.com:5439/dev\n",
      "49 rows affected.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <tr>\n",
       "        <th>i94addr</th>\n",
       "        <th>total_immi</th>\n",
       "        <th>state_code</th>\n",
       "        <th>total_population</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>FL</td>\n",
       "        <td>8156192</td>\n",
       "        <td>FL</td>\n",
       "        <td>32306132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>NY</td>\n",
       "        <td>6764396</td>\n",
       "        <td>NY</td>\n",
       "        <td>49002055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>CA</td>\n",
       "        <td>6531491</td>\n",
       "        <td>CA</td>\n",
       "        <td>123444353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>HI</td>\n",
       "        <td>2338444</td>\n",
       "        <td>HI</td>\n",
       "        <td>1763830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>TX</td>\n",
       "        <td>1690521</td>\n",
       "        <td>TX</td>\n",
       "        <td>70553853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>NV</td>\n",
       "        <td>1387457</td>\n",
       "        <td>NV</td>\n",
       "        <td>11203720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>IL</td>\n",
       "        <td>1085621</td>\n",
       "        <td>IL</td>\n",
       "        <td>22514390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>MA</td>\n",
       "        <td>1057261</td>\n",
       "        <td>MA</td>\n",
       "        <td>9997045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>NJ</td>\n",
       "        <td>993293</td>\n",
       "        <td>NJ</td>\n",
       "        <td>6931024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>WA</td>\n",
       "        <td>822812</td>\n",
       "        <td>WA</td>\n",
       "        <td>12500535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>GA</td>\n",
       "        <td>517612</td>\n",
       "        <td>GA</td>\n",
       "        <td>8555160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>PA</td>\n",
       "        <td>441379</td>\n",
       "        <td>PA</td>\n",
       "        <td>11502801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>MI</td>\n",
       "        <td>415393</td>\n",
       "        <td>MI</td>\n",
       "        <td>10885238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>VA</td>\n",
       "        <td>413904</td>\n",
       "        <td>VA</td>\n",
       "        <td>11818110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>DC</td>\n",
       "        <td>354238</td>\n",
       "        <td>DC</td>\n",
       "        <td>3361140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>CO</td>\n",
       "        <td>331325</td>\n",
       "        <td>CO</td>\n",
       "        <td>14678345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>MD</td>\n",
       "        <td>318308</td>\n",
       "        <td>MD</td>\n",
       "        <td>6560645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>NE</td>\n",
       "        <td>298255</td>\n",
       "        <td>NE</td>\n",
       "        <td>3606165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>NC</td>\n",
       "        <td>290139</td>\n",
       "        <td>NC</td>\n",
       "        <td>15300995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>AZ</td>\n",
       "        <td>271530</td>\n",
       "        <td>AZ</td>\n",
       "        <td>22497710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>OH</td>\n",
       "        <td>261621</td>\n",
       "        <td>OH</td>\n",
       "        <td>12096550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>LA</td>\n",
       "        <td>228854</td>\n",
       "        <td>LA</td>\n",
       "        <td>6502975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>CT</td>\n",
       "        <td>196166</td>\n",
       "        <td>CT</td>\n",
       "        <td>4355096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>OR</td>\n",
       "        <td>192708</td>\n",
       "        <td>OR</td>\n",
       "        <td>7182545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>MN</td>\n",
       "        <td>170047</td>\n",
       "        <td>MN</td>\n",
       "        <td>7044165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>IN</td>\n",
       "        <td>154463</td>\n",
       "        <td>IN</td>\n",
       "        <td>9097794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>TN</td>\n",
       "        <td>151008</td>\n",
       "        <td>TN</td>\n",
       "        <td>10690165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>PR</td>\n",
       "        <td>149110</td>\n",
       "        <td>PR</td>\n",
       "        <td>2030847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>UT</td>\n",
       "        <td>133318</td>\n",
       "        <td>UT</td>\n",
       "        <td>5119677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>WI</td>\n",
       "        <td>121796</td>\n",
       "        <td>WI</td>\n",
       "        <td>7065725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>SC</td>\n",
       "        <td>106721</td>\n",
       "        <td>SC</td>\n",
       "        <td>2586976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>MO</td>\n",
       "        <td>104769</td>\n",
       "        <td>MO</td>\n",
       "        <td>7595970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>AL</td>\n",
       "        <td>100932</td>\n",
       "        <td>AL</td>\n",
       "        <td>5163306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>AK</td>\n",
       "        <td>79539</td>\n",
       "        <td>AK</td>\n",
       "        <td>1493475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>KY</td>\n",
       "        <td>68683</td>\n",
       "        <td>KY</td>\n",
       "        <td>4649385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>NH</td>\n",
       "        <td>56468</td>\n",
       "        <td>NH</td>\n",
       "        <td>990990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>IA</td>\n",
       "        <td>55322</td>\n",
       "        <td>IA</td>\n",
       "        <td>3604003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>RI</td>\n",
       "        <td>51092</td>\n",
       "        <td>RI</td>\n",
       "        <td>1986112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>OK</td>\n",
       "        <td>50393</td>\n",
       "        <td>OK</td>\n",
       "        <td>7244975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>ME</td>\n",
       "        <td>49921</td>\n",
       "        <td>ME</td>\n",
       "        <td>334360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>KS</td>\n",
       "        <td>46691</td>\n",
       "        <td>KS</td>\n",
       "        <td>5741370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>AR</td>\n",
       "        <td>41623</td>\n",
       "        <td>AR</td>\n",
       "        <td>2882889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>DE</td>\n",
       "        <td>40636</td>\n",
       "        <td>DE</td>\n",
       "        <td>359785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>MT</td>\n",
       "        <td>33805</td>\n",
       "        <td>MT</td>\n",
       "        <td>906470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>NM</td>\n",
       "        <td>32667</td>\n",
       "        <td>NM</td>\n",
       "        <td>4195210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>ID</td>\n",
       "        <td>32546</td>\n",
       "        <td>ID</td>\n",
       "        <td>1994415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>MS</td>\n",
       "        <td>25059</td>\n",
       "        <td>MS</td>\n",
       "        <td>1141543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>ND</td>\n",
       "        <td>14734</td>\n",
       "        <td>ND</td>\n",
       "        <td>947450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>SD</td>\n",
       "        <td>10776</td>\n",
       "        <td>SD</td>\n",
       "        <td>1225490</td>\n",
       "    </tr>\n",
       "</table>"
      ],
      "text/plain": [
       "[('FL', 8156192, 'FL', 32306132),\n",
       " ('NY', 6764396, 'NY', 49002055),\n",
       " ('CA', 6531491, 'CA', 123444353),\n",
       " ('HI', 2338444, 'HI', 1763830),\n",
       " ('TX', 1690521, 'TX', 70553853),\n",
       " ('NV', 1387457, 'NV', 11203720),\n",
       " ('IL', 1085621, 'IL', 22514390),\n",
       " ('MA', 1057261, 'MA', 9997045),\n",
       " ('NJ', 993293, 'NJ', 6931024),\n",
       " ('WA', 822812, 'WA', 12500535),\n",
       " ('GA', 517612, 'GA', 8555160),\n",
       " ('PA', 441379, 'PA', 11502801),\n",
       " ('MI', 415393, 'MI', 10885238),\n",
       " ('VA', 413904, 'VA', 11818110),\n",
       " ('DC', 354238, 'DC', 3361140),\n",
       " ('CO', 331325, 'CO', 14678345),\n",
       " ('MD', 318308, 'MD', 6560645),\n",
       " ('NE', 298255, 'NE', 3606165),\n",
       " ('NC', 290139, 'NC', 15300995),\n",
       " ('AZ', 271530, 'AZ', 22497710),\n",
       " ('OH', 261621, 'OH', 12096550),\n",
       " ('LA', 228854, 'LA', 6502975),\n",
       " ('CT', 196166, 'CT', 4355096),\n",
       " ('OR', 192708, 'OR', 7182545),\n",
       " ('MN', 170047, 'MN', 7044165),\n",
       " ('IN', 154463, 'IN', 9097794),\n",
       " ('TN', 151008, 'TN', 10690165),\n",
       " ('PR', 149110, 'PR', 2030847),\n",
       " ('UT', 133318, 'UT', 5119677),\n",
       " ('WI', 121796, 'WI', 7065725),\n",
       " ('SC', 106721, 'SC', 2586976),\n",
       " ('MO', 104769, 'MO', 7595970),\n",
       " ('AL', 100932, 'AL', 5163306),\n",
       " ('AK', 79539, 'AK', 1493475),\n",
       " ('KY', 68683, 'KY', 4649385),\n",
       " ('NH', 56468, 'NH', 990990),\n",
       " ('IA', 55322, 'IA', 3604003),\n",
       " ('RI', 51092, 'RI', 1986112),\n",
       " ('OK', 50393, 'OK', 7244975),\n",
       " ('ME', 49921, 'ME', 334360),\n",
       " ('KS', 46691, 'KS', 5741370),\n",
       " ('AR', 41623, 'AR', 2882889),\n",
       " ('DE', 40636, 'DE', 359785),\n",
       " ('MT', 33805, 'MT', 906470),\n",
       " ('NM', 32667, 'NM', 4195210),\n",
       " ('ID', 32546, 'ID', 1994415),\n",
       " ('MS', 25059, 'MS', 1141543),\n",
       " ('ND', 14734, 'ND', 947450),\n",
       " ('SD', 10776, 'SD', 1225490)]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Possible analysis 2- Check the relationship between total immigration group by states in 2016, compare to total population in the same states.\n",
    "%%sql\n",
    "select * from (select i94addr, sum(count) as total_immi from immigration a group by i94addr)a \n",
    "left join (select state_code, sum(total_population) as total_population from demographic group by state_code) b \n",
    "on a.i94addr = b.state_code\n",
    "where b.state_code is not null order by a.total_immi desc;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.3 Data dictionary \n",
    "Create a data dictionary for your data model. For each field, provide a brief description of what the data is and where it came from. You can include the data dictionary in the notebook or in a separate file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "[Immigration table]:\n",
    "1. immi_id: bigint (Primary key to identify the only record in the table)\n",
    "2. i94yr: int (Year)\n",
    "3. i94mon: int (Month)\n",
    "4. **i94addr: string (State code, for example: New York = NY)**\n",
    "5. gender: string (Gender indication)\n",
    "6. count: bigint (How many immigrations happened)\n",
    "\n",
    "[Demographic table]:\n",
    "1. city_id: bigint (Primary key)\n",
    "2. City: string (City name in the US)\n",
    "3. State: string (states in the US)\n",
    "4. MedianAge: double (MedianAge of the city)\n",
    "5. MalePopulation: bigint (Male Population of the city)\n",
    "6. FemalePopulation: bigint (Female Population of the city)\n",
    "7. TotalPopulation: bigint (Total Population of the city)\n",
    "8. NumberofVeterans: bigint (Number of Veterans of the city)\n",
    "9. ForeignBorn: bigint (ForeignBorn numbers of the city)\n",
    "10. AverageHouseholdSize; double (Average HouseholdSize of the city)\n",
    "11. **StateCode: string (State code in the US, used as join field with immigration table)**\n",
    "12. Race: string (People race)\n",
    "13. Count: bigint (Count of the record)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Step 5: Complete Project Write Up\n",
    "* Clearly state the rationale for the choice of tools and technologies for the project.\n",
    "* Propose how often the data should be updated and why.\n",
    "* Write a description of how you would approach the problem differently under the following scenarios:\n",
    " * The data was increased by 100x.\n",
    " * The data populates a dashboard that must be updated on a daily basis by 7am every day.\n",
    " * The database needed to be accessed by 100+ people."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 1. Clearly state the rationale for the choice of tools and technologies for the project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "1. Spark: The data set of full year 2016 immigration data has 40790529 records, using spark can significantly reduce the data processing time.\n",
    "2. AWS EMR cluster: To corporate with spark to upload the data to S3 using cloud server instead of local computer.\n",
    "3. S3: Data lake storage, to store the data on AWS.\n",
    "4. Redshift: To store the structured tables, and perform queries to do analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 2. Propose how often the data should be updated and why."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "The data update frequency could be monthly, as the least granularity of time is month in this data structure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 3. Write a description of how you would approach the problem differently under the following scenarios:\n",
    " * The data was increased by 100x.\n",
    " * The data populates a dashboard that must be updated on a daily basis by 7am every day.\n",
    " * The database needed to be accessed by 100+ people."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "1. If the data was increased by 100x, then I would still use spark+EMR, but I will provision more nodes in EMR cluster, so it has better persormance. Also I will use parallel data handling when copy data from S3 to Redshift."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "2. I will use airflow to realize the schedule, as airflow is a popular and good tool to schedule flows, and has good integration with AWS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "3. The Redshift can handle max 2000 connections, so 100+ people won't be a problem to Redshift. However, it will need to assign IAM users to different people, and shall have good access/permission control following the least permission granted rules. So need be really carefull when assign edit permission to people."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
